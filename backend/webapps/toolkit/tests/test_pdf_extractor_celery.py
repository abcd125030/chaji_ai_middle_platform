"""
PDFæå–å™¨Celeryä»»åŠ¡æµç¨‹æµ‹è¯•

æµ‹è¯•å®Œæ•´çš„PDFæå–æµç¨‹ï¼š
1. åˆ›å»ºä»»åŠ¡è®°å½•
2. ä¿å­˜PDFæ–‡ä»¶
3. æäº¤Celeryä»»åŠ¡
4. ç­‰å¾…ä»»åŠ¡å®Œæˆ
5. éªŒè¯è¾“å‡ºç»“æœ
"""
import os
import json
import shutil
import logging
import time
import uuid
from datetime import datetime
from pathlib import Path
from django.test import TestCase
from django.conf import settings
from django.db import transaction

from webapps.toolkit.models import PDFExtractorTask
from webapps.toolkit.tasks import process_pdf_extraction
from webapps.toolkit.utils import FileManager


class PDFExtractorCeleryFlowTestCase(TestCase):
    """
    PDFæå–å™¨Celeryæµç¨‹æµ‹è¯•åŸºç±»

    éµå¾ª"åœ¨éš”ç¦»ç¯å¢ƒä¸­é‡å»ºçœŸå®çŠ¶æ€"çš„åŸåˆ™
    """

    @classmethod
    def setUpTestData(cls):
        """
        åœ¨æ•´ä¸ªæµ‹è¯•ç±»è¿è¡Œå‰æ‰§è¡Œä¸€æ¬¡
        å‡†å¤‡æµ‹è¯•æ‰€éœ€çš„é…ç½®æ•°æ®
        """
        print("\n" + "="*70)
        print(f"[{cls.__name__}] Running setUpTestData: Preparing test environment...")

        try:
            with transaction.atomic():
                # è¿™é‡Œå¯ä»¥æ·»åŠ éœ€è¦çš„é…ç½®é¡¹
                # ä¾‹å¦‚ï¼šä»çœŸå®æ•°æ®åº“è¯»å–å¿…è¦é…ç½®å¹¶åˆ›å»ºåˆ°æµ‹è¯•æ•°æ®åº“
                pass

            print(f"[{cls.__name__}] setUpTestData completed successfully.")
        except Exception as e:
            print(f"[{cls.__name__}] CRITICAL: Failed to set up test data: {e}")
            raise
        print("="*70)

    def setUp(self):
        """
        åœ¨æ¯ä¸ªtest_æ–¹æ³•æ‰§è¡Œå‰è¿è¡Œ
        """
        # --- è¾“å‡ºç›®å½•è®¾ç½® ---
        self.output_dir = Path(settings.BASE_DIR) / 'webapps' / 'toolkit' / 'tests' / 'outputs'
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨ç¬¦åˆè§„èŒƒçš„æ–‡ä»¶å‘½åæ ¼å¼
        test_method_name = self._testMethodName
        timestamp = datetime.now()
        date_str = timestamp.strftime('%Y%m%d')
        time_str = timestamp.strftime('%H%M%S')

        # ç”Ÿæˆç¬¦åˆè§„èŒƒçš„æ–‡ä»¶å
        self.process_filename = f'process-{date_str}-{time_str}-{test_method_name}.json'
        self.log_filename = f'log-{date_str}-{time_str}-{test_method_name}.log'
        self.result_filename = f'result-{date_str}-{time_str}-{test_method_name}.json'

        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        self.logger.info(f"Test method '{test_method_name}' starting...")

        # --- çŠ¶æ€æ•è·å’Œæ•°æ®è®°å½• ---
        self.initial_state = self.capture_state("initial")
        self.process_data = {
            "test_info": {
                "name": f"{self.__class__.__name__}.{test_method_name}",
                "start_time": datetime.now().isoformat()
            },
            "initial_state": self.initial_state,
            "execution_steps": []
        }

        # --- æµ‹è¯•æ–‡ä»¶è·¯å¾„ ---
        self.test_pdf_path = Path(settings.BASE_DIR) / 'webapps' / 'toolkit' / 'exp' / 'Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models.pdf'

        if not self.test_pdf_path.exists():
            raise FileNotFoundError(f"æµ‹è¯•PDFæ–‡ä»¶ä¸å­˜åœ¨: {self.test_pdf_path}")

        self.logger.info(f"æµ‹è¯•PDFæ–‡ä»¶: {self.test_pdf_path}")

    def setup_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•å™¨"""
        log_file = self.output_dir / self.log_filename

        # é¿å…é‡å¤æ·»åŠ handler
        self.logger = logging.getLogger(self.log_filename)
        if not self.logger.handlers:
            self.logger.setLevel(logging.DEBUG)
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

            # æ–‡ä»¶å¤„ç†å™¨
            fh = logging.FileHandler(log_file, encoding='utf-8')
            fh.setFormatter(formatter)
            self.logger.addHandler(fh)

            # æ§åˆ¶å°å¤„ç†å™¨
            sh = logging.StreamHandler()
            sh.setFormatter(formatter)
            self.logger.addHandler(sh)

    def capture_state(self, stage: str):
        """
        æ•è·å½“å‰æ•°æ®åº“çŠ¶æ€

        Args:
            stage: 'initial' æˆ– 'final'
        """
        self.logger.info(f"Capturing {stage} state...")

        # æ•è·PDFExtractorTaskè¡¨çŠ¶æ€
        tasks_snapshot = list(PDFExtractorTask.objects.values(
            'id', 'original_filename', 'status', 'total_pages', 'processed_pages'
        ))

        return {
            "database": {
                "PDFExtractorTask_count": PDFExtractorTask.objects.count(),
                "PDFExtractorTask_records": tasks_snapshot
            }
        }

    def record_step(self, action, input_data, output_data, **kwargs):
        """è®°å½•æ‰§è¡Œæ­¥éª¤"""
        step_number = len(self.process_data["execution_steps"]) + 1
        step_data = {
            "step": step_number,
            "action": action,
            "input": str(input_data)[:500],  # é™åˆ¶é•¿åº¦
            "output": str(output_data)[:500],  # é™åˆ¶é•¿åº¦
            "timestamp": datetime.now().isoformat(),
            **kwargs
        }
        self.process_data["execution_steps"].append(step_data)
        self.logger.info(f"Step {step_number}: {action} recorded.")

    def tearDown(self):
        """
        åœ¨æ¯ä¸ªtest_æ–¹æ³•æ‰§è¡Œåè¿è¡Œ
        """
        end_time = datetime.now()
        start_time_iso = self.process_data["test_info"]["start_time"]
        start_time = datetime.fromisoformat(start_time_iso)
        duration = (end_time - start_time).total_seconds()

        self.process_data["test_info"]["end_time"] = end_time.isoformat()
        self.process_data["test_info"]["duration"] = duration
        self.process_data["final_state"] = self.capture_state("final")

        # ä¿å­˜è¿‡ç¨‹æ•°æ®
        process_file = self.output_dir / self.process_filename
        try:
            with open(process_file, 'w', encoding='utf-8') as f:
                json.dump(self.process_data, f, ensure_ascii=False, indent=2, default=str)
        except Exception as e:
            self.logger.error(f"Failed to write process file: {e}")

        self.logger.info(f"Test method finished. Duration: {duration:.2f}s")
        print(f"\n[INFO] Test outputs saved to directory: {self.output_dir.absolute()}")


class TestPDFExtractorCeleryFlow(PDFExtractorCeleryFlowTestCase):
    """æµ‹è¯•PDFæå–å™¨å®Œæ•´Celeryæµç¨‹"""

    def test_complete_pdf_extraction_flow(self):
        """
        æµ‹è¯•å®Œæ•´çš„PDFæå–æµç¨‹

        æ­¥éª¤ï¼š
        1. åˆ›å»ºä»»åŠ¡UUIDå’Œç›®å½•
        2. å¤åˆ¶æµ‹è¯•PDFåˆ°ä»»åŠ¡ç›®å½•
        3. åˆ›å»ºæ•°æ®åº“ä»»åŠ¡è®°å½•
        4. åŒæ­¥è°ƒç”¨Celeryä»»åŠ¡ï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰
        5. éªŒè¯ä»»åŠ¡çŠ¶æ€
        6. éªŒè¯è¾“å‡ºæ–‡ä»¶
        7. éªŒè¯ç»“æœå†…å®¹
        """
        self.logger.info("="*70)
        self.logger.info("å¼€å§‹æµ‹è¯•: å®Œæ•´PDFæå–æµç¨‹")
        self.logger.info("="*70)

        # ==================== æ­¥éª¤1: åˆ›å»ºä»»åŠ¡UUIDå’Œç›®å½• ====================
        task_id = str(uuid.uuid4())
        task_dir = FileManager.create_task_directory(task_id)

        self.record_step(
            action="åˆ›å»ºä»»åŠ¡ç›®å½•",
            input_data={"task_id": task_id},
            output_data={"task_dir": str(task_dir)}
        )
        self.logger.info(f"âœ… æ­¥éª¤1å®Œæˆ: ä»»åŠ¡ID={task_id}, ç›®å½•={task_dir}")

        # ==================== æ­¥éª¤2: å¤åˆ¶æµ‹è¯•PDFåˆ°ä»»åŠ¡ç›®å½• ====================
        pdf_filename = f"{task_id}.pdf"
        pdf_path = task_dir / pdf_filename
        shutil.copy(self.test_pdf_path, pdf_path)

        self.record_step(
            action="å¤åˆ¶PDFæ–‡ä»¶",
            input_data={"source": str(self.test_pdf_path)},
            output_data={"destination": str(pdf_path), "size_mb": pdf_path.stat().st_size / 1024 / 1024}
        )
        self.logger.info(f"âœ… æ­¥éª¤2å®Œæˆ: PDFå·²å¤åˆ¶åˆ° {pdf_path}")

        # ==================== æ­¥éª¤3: åˆ›å»ºæ•°æ®åº“ä»»åŠ¡è®°å½• ====================
        task = PDFExtractorTask.objects.create(
            id=task_id,
            original_filename=self.test_pdf_path.name,
            file_path=str(pdf_path),
            status='pending'
        )

        self.record_step(
            action="åˆ›å»ºä»»åŠ¡è®°å½•",
            input_data={
                "task_id": str(task_id),
                "original_filename": self.test_pdf_path.name,
                "file_path": str(pdf_path)
            },
            output_data={"task_status": task.status}
        )
        self.logger.info(f"âœ… æ­¥éª¤3å®Œæˆ: ä»»åŠ¡è®°å½•å·²åˆ›å»º, çŠ¶æ€={task.status}")

        # ==================== æ­¥éª¤4: åŒæ­¥è°ƒç”¨Celeryä»»åŠ¡ ====================
        self.logger.info("å¼€å§‹æ‰§è¡ŒCeleryä»»åŠ¡ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰...")
        start_processing_time = datetime.now()

        try:
            # åœ¨æµ‹è¯•ç¯å¢ƒä¸­åŒæ­¥è°ƒç”¨ä»»åŠ¡
            result = process_pdf_extraction(str(task.id), str(pdf_path))

            processing_duration = (datetime.now() - start_processing_time).total_seconds()

            self.record_step(
                action="æ‰§è¡ŒCeleryä»»åŠ¡",
                input_data={"task_id": str(task_id), "pdf_path": str(pdf_path)},
                output_data={"result": result, "duration": processing_duration}
            )
            self.logger.info(f"âœ… æ­¥éª¤4å®Œæˆ: Celeryä»»åŠ¡æ‰§è¡Œå®Œæˆ, è€—æ—¶={processing_duration:.2f}ç§’")

        except Exception as e:
            self.logger.error(f"âŒ Celeryä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            self.record_step(
                action="æ‰§è¡ŒCeleryä»»åŠ¡",
                input_data={"task_id": str(task_id)},
                output_data={"error": str(e)},
                status="failed"
            )
            raise

        # ==================== æ­¥éª¤5: éªŒè¯ä»»åŠ¡çŠ¶æ€ ====================
        task.refresh_from_db()

        self.logger.info(f"ä»»åŠ¡çŠ¶æ€: {task.status}")
        self.logger.info(f"æ€»é¡µæ•°: {task.total_pages}")
        self.logger.info(f"å·²å¤„ç†é¡µæ•°: {task.processed_pages}")

        self.record_step(
            action="éªŒè¯ä»»åŠ¡çŠ¶æ€",
            input_data={"task_id": str(task_id)},
            output_data={
                "status": task.status,
                "total_pages": task.total_pages,
                "processed_pages": task.processed_pages
            }
        )

        # æ–­è¨€ä»»åŠ¡å®Œæˆ
        self.assertIn(task.status, ['completed', 'completed_with_errors'],
                     f"ä»»åŠ¡çŠ¶æ€åº”ä¸ºcompletedæˆ–completed_with_errorsï¼Œå®é™…ä¸º: {task.status}")
        self.assertGreater(task.total_pages, 0, "æ€»é¡µæ•°åº”å¤§äº0")
        self.assertEqual(task.processed_pages, task.total_pages,
                        f"å·²å¤„ç†é¡µæ•°({task.processed_pages})åº”ç­‰äºæ€»é¡µæ•°({task.total_pages})")

        self.logger.info("âœ… æ­¥éª¤5å®Œæˆ: ä»»åŠ¡çŠ¶æ€éªŒè¯é€šè¿‡")

        # ==================== æ­¥éª¤6: éªŒè¯è¾“å‡ºæ–‡ä»¶ç»“æ„ ====================
        self.logger.info("æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ç»“æ„...")

        expected_files = []

        # æ£€æŸ¥task.json
        task_json_path = task_dir / 'task.json'
        if task_json_path.exists():
            expected_files.append(str(task_json_path))
            with open(task_json_path, 'r', encoding='utf-8') as f:
                task_json_data = json.load(f)
            self.logger.info(f"  âœ“ task.jsonå­˜åœ¨: {task_json_data.get('status')}")

        # æ£€æŸ¥æœ€ç»ˆmarkdown
        final_md_path = task_dir / f"{task_id}_result.md"
        if final_md_path.exists():
            expected_files.append(str(final_md_path))
            md_size = final_md_path.stat().st_size
            self.logger.info(f"  âœ“ æœ€ç»ˆmarkdownå­˜åœ¨: {final_md_path.name}, å¤§å°={md_size}å­—èŠ‚")

        # æ£€æŸ¥é¡µé¢ç›®å½•
        page_dirs = sorted(task_dir.glob('page_*'))
        for page_dir in page_dirs:
            if page_dir.is_dir():
                self.logger.info(f"  âœ“ é¡µé¢ç›®å½•: {page_dir.name}")

                # æ£€æŸ¥é¡µé¢æ–‡ä»¶
                page_files = list(page_dir.glob('*'))
                for page_file in page_files:
                    expected_files.append(str(page_file))
                    self.logger.info(f"    - {page_file.name}")

        self.record_step(
            action="éªŒè¯è¾“å‡ºæ–‡ä»¶",
            input_data={"task_dir": str(task_dir)},
            output_data={
                "total_files": len(expected_files),
                "page_dirs": len(page_dirs),
                "files": expected_files[:20]  # é™åˆ¶è®°å½•æ•°é‡
            }
        )

        # æ–­è¨€å…³é”®æ–‡ä»¶å­˜åœ¨
        self.assertTrue(task_json_path.exists(), "task.jsonåº”è¯¥å­˜åœ¨")
        self.assertTrue(final_md_path.exists(), "æœ€ç»ˆmarkdownæ–‡ä»¶åº”è¯¥å­˜åœ¨")
        self.assertGreater(len(page_dirs), 0, "åº”è¯¥è‡³å°‘æœ‰ä¸€ä¸ªé¡µé¢ç›®å½•")

        self.logger.info(f"âœ… æ­¥éª¤6å®Œæˆ: è¾“å‡ºæ–‡ä»¶ç»“æ„éªŒè¯é€šè¿‡, å…±{len(expected_files)}ä¸ªæ–‡ä»¶")

        # ==================== æ­¥éª¤7: ä¿å­˜æµ‹è¯•ç»“æœ ====================
        test_result = {
            "test_name": "test_complete_pdf_extraction_flow",
            "task_id": str(task_id),
            "status": "success",
            "task_status": task.status,
            "total_pages": task.total_pages,
            "processed_pages": task.processed_pages,
            "processing_duration": processing_duration,
            "output_files_count": len(expected_files),
            "task_dir": str(task_dir),
            "final_markdown": str(final_md_path) if final_md_path.exists() else None
        }

        result_file = self.output_dir / self.result_filename
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2, default=str)

        self.logger.info("="*70)
        self.logger.info("âœ… æµ‹è¯•å®Œæˆ: æ‰€æœ‰éªŒè¯é€šè¿‡")
        self.logger.info(f"ğŸ“ ä»»åŠ¡ç›®å½•: {task_dir}")
        self.logger.info(f"ğŸ“„ æœ€ç»ˆç»“æœ: {final_md_path}")
        self.logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Š: {result_file}")
        self.logger.info("="*70)
