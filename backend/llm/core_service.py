import json
import requests
import logging
from typing import Dict, Any, Optional, List, Union, Generator
import time
import base64
from pathlib import Path

from .retry_utils import LLMRetryHandler, RetryConfig
from .log_service import LLMLogService

logger = logging.getLogger(__name__)

class CoreLLMService:
    """
    çº¯å‡€çš„LLMè°ƒç”¨æœåŠ¡
    åªè´Ÿè´£è°ƒç”¨LLM APIï¼Œä¸æ¶‰åŠé‰´æƒå’Œæƒé™
    """
    
    def __init__(self):
        self._request_cache = {}
    
    def call_llm(self, model_id: str, endpoint: str, api_key: str, 
                 messages: List[Dict], custom_headers: Optional[Dict] = None, 
                 params: Optional[Dict] = None, 
                 user=None, session_id: str = None,
                 source_app: str = None, source_function: str = None,
                 model_name: str = None, vendor_name: str = None, 
                 vendor_id: str = None, enable_logging: bool = True,
                 **kwargs) -> Union[Dict, Generator]:
        """
        çº¯å‡€çš„LLM APIè°ƒç”¨
        å‚æ•°ï¼šæ¨¡å‹é…ç½®ä¿¡æ¯ + è°ƒç”¨å‚æ•°
        è¿”å›ï¼šLLMå“åº” (æ”¯æŒæµå¼å’Œéæµå¼)
        
        æ–°å¢å‚æ•°:
            user: ç”¨æˆ·å¯¹è±¡ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            session_id: ä¼šè¯ID
            source_app: æ¥æºåº”ç”¨
            source_function: æ¥æºå‡½æ•°
            model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨model_idï¼‰
            vendor_name: ä¾›åº”å•†åç§°
            vendor_id: ä¾›åº”å•†æ ‡è¯†
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—è®°å½•
        """
        headers = {'Content-Type': 'application/json'}
        
        # è®¾ç½®API key
        if api_key:
            headers['Authorization'] = f'Bearer {api_key}'
        
        # åˆå¹¶è‡ªå®šä¹‰headers
        if custom_headers:
            headers.update(custom_headers)
        
        # æ„å»ºpayload
        payload = {
            'model': model_id,
            'messages': messages,
            **(params or {}),
            **kwargs
        }
        
        # æ‰“å°å®Œæ•´çš„LLMè¯·æ±‚ä¿¡æ¯ï¼ˆå•ä¸ªloggerè°ƒç”¨ï¼Œç”¨äºè°ƒè¯•ï¼‰
        messages_debug = []
        for i, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªæ–­æ˜¾ç¤º
            if len(content) > 3000:
                content_display = f"{content}\næ€»é•¿åº¦: {len(content)} å­—ç¬¦]"
            else:
                content_display = content
            messages_debug.append(f"ã€Message {i+1} - Role: {role}ã€‘\n{content_display}")
        
        llm_request_debug = f"""
{"=" * 60}
ğŸ¤– LLM è°ƒç”¨è¯·æ±‚
æ¨¡å‹: {model_id}
ç«¯ç‚¹: {endpoint}
æµå¼: {payload.get('stream', False)}
{"=" * 60}

{chr(10).join(messages_debug)}

{"=" * 60}
"""
        # logger.info(llm_request_debug)
        
        # ç‰¹æ®Šå¤„ç†
        if model_id == "qwq-32b":
            payload['stream'] = True
        
        # OpenRouterç‰¹æ®Šheaders
        if endpoint == "https://openrouter.ai/api/v1/chat/completions":
            headers.update({
                'HTTP-Referer': "https://chagee.com",
                'X-Title': "Internal Service"
            })

        # åˆ›å»ºæ—¥å¿—è®°å½•
        log_entry = None
        if enable_logging:
            log_entry = LLMLogService.create_call_log(
                model_name=model_id,  # ç»Ÿä¸€ä½¿ç”¨ model_id ä½œä¸º model_name
                model_id=model_id,
                endpoint=endpoint,
                messages=messages,
                params={**params, **kwargs} if params else kwargs,
                headers=headers,
                user=user,
                session_id=session_id,
                call_type='structured' if 'output_schema' in kwargs else 'chat',
                source_app=source_app or 'llm',
                source_function=source_function or 'core_service.call_llm',
                vendor_name=vendor_name,
                vendor_id=vendor_id,
                is_stream=payload.get('stream', False),
                metadata={}
            )
        
        # åˆ›å»ºé‡è¯•é…ç½®
        retry_config = RetryConfig(
            max_attempts=3,
            initial_delay=1.0,
            max_delay=10.0,
            exponential_base=2.0,
            jitter=True
        )
        
        # å®šä¹‰è¯·æ±‚å‡½æ•°
        def make_request():
            response = requests.post(
                endpoint,
                headers=headers,
                json=payload,
                timeout=300,
                stream=payload.get('stream', False)
            )
            response.raise_for_status()
            return response
        
        try:
            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡Œè¯·æ±‚
            response = LLMRetryHandler.retry_with_backoff(
                make_request,
                retry_config,
                on_retry=lambda attempt, error, delay: self._on_retry(
                    log_entry, attempt, error, delay
                )
            )
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼å“åº”
            is_stream = payload.get('stream', False)
            is_sse = 'text/event-stream' in response.headers.get('Content-Type', '')
            
            if is_stream and is_sse:
                logger.info("è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨")
                return self._handle_stream_response(response, log_entry)
            else:
                # æ‰“å°å“åº”ç»“æœ
                response_data = response.json()
                content = response_data.get('choices', [{}])[0].get('message', {}).get('content', '')
                
                # å‡†å¤‡å“åº”æ—¥å¿—
                if len(content) > 3000:
                    content_display = f"{content}\næ€»é•¿åº¦: {len(content)} å­—ç¬¦]"
                else:
                    content_display = content
                
                usage = response_data.get('usage', {})
                llm_response_debug = f"""
{"=" * 60}
âœ… LLM å“åº”ç»“æœ
æ¨¡å‹: {model_id}
å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦
Tokenä½¿ç”¨: è¾“å…¥ {usage.get('prompt_tokens', 'N/A')} | è¾“å‡º {usage.get('completion_tokens', 'N/A')} | æ€»è®¡ {usage.get('total_tokens', 'N/A')}
{"=" * 60}

ã€å“åº”å†…å®¹ã€‘
{content_display}

{"=" * 60}
"""
                # logger.debug(llm_response_debug)
                
                # æ›´æ–°æ—¥å¿—è®°å½•
                if log_entry:
                    usage = response_data.get('usage', {})
                    LLMLogService.update_success(
                        log_entry,
                        response_content=content,
                        response_raw=response_data,
                        usage_data=usage
                    )
                
                return response_data
                
        except requests.exceptions.Timeout:
            error_desc, _ = LLMRetryHandler.get_error_description(Exception("Timeout"))
            logger.info(f"LLMæœåŠ¡è°ƒç”¨å¤±è´¥: {error_desc}")
            if log_entry:
                LLMLogService.update_timeout(log_entry)
            raise Exception(error_desc)
        except requests.exceptions.RequestException as e:
            error_desc, _ = LLMRetryHandler.get_error_description(e)
            logger.info(f"LLMæœåŠ¡è°ƒç”¨å¤±è´¥: {error_desc}")
            if log_entry:
                LLMLogService.update_failure(log_entry, error_desc)
            raise Exception(error_desc)
        except Exception as e:
            error_desc, _ = LLMRetryHandler.get_error_description(e)
            logger.info(f"LLMæœåŠ¡è°ƒç”¨å¼‚å¸¸: {error_desc}")
            if log_entry:
                LLMLogService.update_failure(log_entry, str(e))
            # å¦‚æœå¼‚å¸¸ä¿¡æ¯å·²ç»æ˜¯å‹å¥½æè¿°ï¼Œç›´æ¥ä½¿ç”¨
            if any(desc in str(e) for desc in LLMRetryHandler.RETRYABLE_ERRORS.values()):
                raise
            else:
                raise Exception(error_desc)
    
    def _handle_stream_response(self, response, log_entry=None) -> Generator:
        """å¤„ç†æµå¼å“åº”"""
        full_text = ""
        response_template = None
        usage_data = {}
        
        try:
            for line in response.iter_lines():
                if line:
                    line_str = line.lstrip(b'data: ').strip()
                    if not line_str or line_str == b'[DONE]':
                        continue
                    
                    yield f"data: {line_str.decode('utf-8')}\n\n"
                    
                    try:
                        chunk = json.loads(line_str)
                        if response_template is None:
                            response_template = chunk
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯
                        if chunk.get("usage"):
                            usage_data = chunk.get("usage")
                        
                        delta = chunk.get("choices", [{}])[0].get("delta", {})
                        content = delta.get("content")
                        if content:
                            full_text += content
                    except json.JSONDecodeError:
                        continue
        finally:
            # æ›´æ–°æ—¥å¿—è®°å½•
            if log_entry and full_text:
                # å¦‚æœæ²¡æœ‰usageä¿¡æ¯ï¼Œå°è¯•ä¼°ç®—
                if not usage_data:
                    try:
                        import tiktoken
                        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
                        prompt_tokens = sum(len(encoding.encode(msg.get('content', ''))) 
                                          for msg in log_entry.request_messages if msg.get('content'))
                        completion_tokens = len(encoding.encode(full_text))
                        usage_data = {
                            'prompt_tokens': prompt_tokens,
                            'completion_tokens': completion_tokens,
                            'total_tokens': prompt_tokens + completion_tokens
                        }
                    except:
                        pass
                
                LLMLogService.update_success(
                    log_entry,
                    response_content=full_text,
                    response_raw=response_template or {},
                    usage_data=usage_data
                )
            
            # å‘é€ç»“æŸä¿¡å·
            yield "data: [DONE]\n\n"
    
    def _on_retry(self, log_entry, attempt, error, delay):
        """é‡è¯•æ—¶çš„å›è°ƒ"""
        logger.info(f"LLMè¯·æ±‚ç¬¬{attempt}æ¬¡å°è¯•å¤±è´¥ï¼Œ{delay:.2f}ç§’åé‡è¯•")
        if log_entry:
            LLMLogService.update_retry(log_entry, attempt)
    
    def call_vision_llm(self, model_id: str, endpoint: str, api_key: str,
                       text_prompt: str, images: List[str],
                       custom_headers: Optional[Dict] = None,
                       params: Optional[Dict] = None,
                       user=None, session_id: str = None,
                       source_app: str = None, source_function: str = None,
                       model_name: str = None, vendor_name: str = None,
                       vendor_id: str = None, enable_logging: bool = True,
                       system_prompt: Optional[str] = None,
                       **kwargs) -> Union[Dict, Generator]:
        """
        è§†è§‰æ¨¡å‹ä¸“ç”¨è°ƒç”¨æ–¹æ³•
        
        å‚æ•°:
            model_id: æ¨¡å‹ID
            endpoint: APIç«¯ç‚¹
            api_key: APIå¯†é’¥
            text_prompt: æ–‡æœ¬æç¤ºè¯
            images: å›¾ç‰‡åˆ—è¡¨ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
                - base64å­—ç¬¦ä¸² (data:image/jpeg;base64,...)
                - HTTP/HTTPS URL
                - æœ¬åœ°æ–‡ä»¶è·¯å¾„
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
            å…¶ä»–å‚æ•°åŒ call_llm æ–¹æ³•
        
        è¿”å›:
            LLMå“åº” (æ”¯æŒæµå¼å’Œéæµå¼)
        """
        try:
            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            messages = self._build_vision_messages(text_prompt, images, system_prompt)
            
            # è®°å½•è§†è§‰æ¨¡å‹è°ƒç”¨ä¿¡æ¯
            logger.info(f"""
è§†è§‰æ¨¡å‹è°ƒç”¨:
- æ¨¡å‹: {model_id}
- æ–‡æœ¬æç¤º: {text_prompt[:100]}...
- å›¾ç‰‡æ•°é‡: {len(images)}
- ç³»ç»Ÿæç¤º: {'æœ‰' if system_prompt else 'æ— '}
""")
            
            # å¤ç”¨ç°æœ‰çš„ call_llm æ–¹æ³•
            return self.call_llm(
                model_id=model_id,
                endpoint=endpoint,
                api_key=api_key,
                messages=messages,
                custom_headers=custom_headers,
                params=params,
                user=user,
                session_id=session_id,
                source_app=source_app or 'llm',
                source_function=source_function or 'core_service.call_vision_llm',
                model_name=model_name,
                vendor_name=vendor_name,
                vendor_id=vendor_id,
                enable_logging=enable_logging,
                **kwargs
            )
        except Exception as e:
            logger.error(f"è§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _build_vision_messages(self, text: str, images: List[str], 
                               system_prompt: Optional[str] = None) -> List[Dict]:
        """
        æ„å»ºè§†è§‰æ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼
        æ”¯æŒOpenAIæ ‡å‡†æ ¼å¼å’Œå…¶ä»–ä¸»æµæ ¼å¼
        
        å‚æ•°:
            text: æ–‡æœ¬æç¤ºè¯
            images: å›¾ç‰‡åˆ—è¡¨
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
        
        è¿”å›:
            ç¬¦åˆè§†è§‰æ¨¡å‹è¦æ±‚çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
        content = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹
        if text:
            content.append({
                "type": "text",
                "text": text
            })
        
        # å¤„ç†æ¯ä¸ªå›¾ç‰‡
        for i, image in enumerate(images):
            try:
                image_url = self._process_image_input(image)
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": image_url
                    }
                })
                logger.debug(f"å›¾ç‰‡ {i+1} å·²å¤„ç†: {image_url[:100]}...")
            except Exception as e:
                logger.warning(f"å¤„ç†å›¾ç‰‡ {i+1} å¤±è´¥: {str(e)}")
                # å¯ä»¥é€‰æ‹©è·³è¿‡å¤±è´¥çš„å›¾ç‰‡æˆ–æŠ›å‡ºå¼‚å¸¸
                continue
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": content
        })
        
        return messages
    
    def _process_image_input(self, image: str) -> str:
        """
        å¤„ç†ä¸åŒæ ¼å¼çš„å›¾ç‰‡è¾“å…¥
        
        å‚æ•°:
            image: å›¾ç‰‡è¾“å…¥ï¼Œå¯ä»¥æ˜¯:
                - base64å­—ç¬¦ä¸² (data:image/...)
                - HTTP/HTTPS URL
                - æœ¬åœ°æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
            å¤„ç†åçš„å›¾ç‰‡URL (base64æˆ–HTTP URL)
        """
        # å¦‚æœå·²ç»æ˜¯data URLæˆ–HTTP URLï¼Œç›´æ¥è¿”å›
        if image.startswith('data:image/') or image.startswith(('http://', 'https://')):
            return image
        
        # å¦åˆ™å‡å®šæ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œè½¬æ¢ä¸ºbase64
        try:
            return self._file_to_base64(image)
        except Exception as e:
            raise ValueError(f"æ— æ³•å¤„ç†å›¾ç‰‡è¾“å…¥: {str(e)}")
    
    def _file_to_base64(self, file_path: str) -> str:
        """
        å°†æœ¬åœ°æ–‡ä»¶è½¬æ¢ä¸ºbase64æ ¼å¼çš„data URL
        
        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
            base64æ ¼å¼çš„data URL
        """
        path = Path(file_path)
        
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not path.is_file():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}")
        
        # è·å–æ–‡ä»¶æ‰©å±•åå’ŒMIMEç±»å‹
        suffix = path.suffix.lower()
        mime_types = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp',
            '.bmp': 'image/bmp'
        }
        
        mime_type = mime_types.get(suffix, 'image/jpeg')
        
        # è¯»å–æ–‡ä»¶å¹¶è½¬æ¢ä¸ºbase64
        with open(path, 'rb') as f:
            image_data = f.read()
            base64_data = base64.b64encode(image_data).decode('utf-8')
        
        return f"data:{mime_type};base64,{base64_data}"
    
    def get_structured_llm(self, output_schema, model_config: Dict, **kwargs):
        """
        è¿”å›ç»“æ„åŒ–LLMè°ƒç”¨å™¨
        
        å‚æ•°:
            output_schema: è¾“å‡ºæ¨¡å¼å®šä¹‰
            model_config: æ¨¡å‹é…ç½®ä¿¡æ¯
            **kwargs: é¢å¤–å‚æ•°ï¼ˆuser, session_id, model_nameç­‰ï¼‰
        """
        return StructuredLLMClient(
            core_service=self,
            output_schema=output_schema,
            **model_config,
            **kwargs
        )


class StructuredLLMClient:
    """ç»“æ„åŒ–LLMè°ƒç”¨å®¢æˆ·ç«¯"""
    
    def __init__(self, core_service: CoreLLMService, output_schema, 
                 model_id: str, endpoint: str, api_key: str, 
                 custom_headers: Optional[Dict] = None, 
                 params: Optional[Dict] = None, 
                 user=None, session_id: str = None,
                 model_name: str = None, vendor_name: str = None,
                 vendor_id: str = None, source_app: str = None,
                 source_function: str = None, **kwargs):
        self.core_service = core_service
        self.output_schema = output_schema
        self.config = {
            'model_id': model_id,
            'endpoint': endpoint,
            'api_key': api_key,
            'custom_headers': custom_headers,
            'params': params
        }
        # ä¿å­˜æ—¥å¿—ç›¸å…³å‚æ•°
        self.log_params = {
            'user': user,
            'session_id': session_id,
            'model_name': model_name,
            'vendor_name': vendor_name,
            'vendor_id': vendor_id,
            'source_app': source_app,
            'source_function': source_function,
            'enable_logging': True
        }
    
    def invoke(self, prompt: str, system_prompt: Optional[str] = None):
        """
        è°ƒç”¨LLMå¹¶è¿”å›ç»“æ„åŒ–è¾“å‡º
        
        å‚æ•°:
            prompt: ç”¨æˆ·æç¤ºè¯å†…å®¹
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯ï¼Œç”¨äºè®¾ç½®LLMçš„è¡Œä¸ºè§„èŒƒ
        """
        # æ„å»ºç»“æ„åŒ–æç¤ºè¯
        schema_json = self.output_schema.model_json_schema()
        schema_str = json.dumps(schema_json, indent=2, ensure_ascii=False)
        
        # ç®€åŒ–çš„æ¶ˆæ¯æ„å»ºé€»è¾‘
        if system_prompt:
            # æœ‰ç³»ç»Ÿæç¤ºè¯æ—¶ï¼Œä½¿ç”¨ system role + user role
            messages = [
                {
                    'role': 'system', 
                    'content': f"{system_prompt}\n\nOutput JSON Schema:\n{schema_str}\n\nIMPORTANT: Output ONLY valid JSON matching the schema above. No explanations, no markdown."
                },
                {'role': 'user', 'content': prompt}
            ]
        else:
            # æ— ç³»ç»Ÿæç¤ºè¯æ—¶ï¼Œåˆå¹¶åˆ° user role
            messages = [
                {
                    'role': 'user',
                    'content': f"{prompt}\n\nOutput as JSON following this schema:\n{schema_str}\n\nIMPORTANT: Output ONLY valid JSON. No explanations, no markdown formatting."
                }
            ]
        
        # è°ƒç”¨æ ¸å¿ƒLLMæœåŠ¡ (å¼ºåˆ¶éæµå¼)ï¼Œä¼ å…¥æ—¥å¿—ç›¸å…³å‚æ•°
        response = self.core_service.call_llm(
            messages=messages,
            temperature=0.75,  # ç»“æ„åŒ–è¾“å‡ºéœ€è¦ä½æ¸©åº¦
            stream=False,  # ç»“æ„åŒ–è¾“å‡ºä¸æ”¯æŒæµå¼
            **self.config,
            **self.log_params  # ä¼ å…¥æ—¥å¿—ç›¸å…³å‚æ•°
        )
        
        # è§£æç»“æ„åŒ–è¾“å‡º
        return self._parse_structured_response(response)
    
    def _parse_structured_response(self, response: Dict) -> Any:
        """
        è§£æLLMå“åº”å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®
        
        å‚æ•°:
            response: LLMåŸå§‹å“åº”
        è¿”å›:
            è§£æåçš„Pydanticæ¨¡å‹å®ä¾‹
        """
        raw_response_text = None
        
        try:
            # è·å–åŸå§‹å“åº”æ–‡æœ¬
            raw_response_text = response['choices'][0]['message']['content']
            
            # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œæå–JSON
            json_str = self._extract_json_from_response(raw_response_text)
            
            # è§£æJSONå¹¶è¿”å›æ¨¡å‹å®ä¾‹
            data = json.loads(json_str)
            return self.output_schema(**data)
            
        except json.JSONDecodeError as e:
            logger.error(f"ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥: {e}")
            # å°è¯•æ›´æ¿€è¿›çš„æ¸…ç†
            if raw_response_text:
                json_str = self._aggressive_json_cleanup(raw_response_text)
                try:
                    data = json.loads(json_str)
                    return self.output_schema(**data)
                except Exception as final_e:
                    logger.error(f"æ¿€è¿›æ¸…ç†åä»è§£æå¤±è´¥: {final_e}")
        except Exception as e:
            logger.error(f"ç»“æ„åŒ–è¾“å‡ºå¤„ç†å¤±è´¥: {e}")
        
        # æ— è®ºä»€ä¹ˆå¼‚å¸¸ï¼Œéƒ½è¿”å›é»˜è®¤çš„æ¨¡å‹å®ä¾‹è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        # åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤å€¼çš„å®ä¾‹
        try:
            # å°è¯•åˆ›å»ºå¸¦æœ‰é”™è¯¯ä¿¡æ¯çš„é»˜è®¤å®ä¾‹
            # è·å–æ¨¡å‹çš„æ‰€æœ‰å­—æ®µ
            schema_fields = self.output_schema.model_fields
            default_data = {}
            
            for field_name, field_info in schema_fields.items():
                # ä¸ºæ¯ä¸ªå­—æ®µè®¾ç½®åˆé€‚çš„é»˜è®¤å€¼
                if field_info.is_required():
                    # å¿…å¡«å­—æ®µè®¾ç½®é»˜è®¤å€¼
                    if field_info.annotation == str:
                        default_data[field_name] = f"[è§£æå¤±è´¥] åŸå§‹å“åº”: {raw_response_text[:200] if raw_response_text else 'æ— å“åº”'}"
                    elif field_info.annotation == int:
                        default_data[field_name] = 0
                    elif field_info.annotation == float:
                        default_data[field_name] = 0.0
                    elif field_info.annotation == bool:
                        default_data[field_name] = False
                    elif field_info.annotation == list:
                        default_data[field_name] = []
                    elif field_info.annotation == dict:
                        default_data[field_name] = {}
                    else:
                        # å¯¹äºå¤æ‚ç±»å‹ï¼Œå°è¯•ä½¿ç”¨Noneæˆ–ç©ºå­—å…¸
                        default_data[field_name] = None if not field_info.is_required() else {}
            
            return self.output_schema(**default_data)
        except Exception as create_e:
            logger.error(f"åˆ›å»ºé»˜è®¤å®ä¾‹å¤±è´¥: {create_e}")
            # æœ€åçš„fallbackï¼šä½¿ç”¨ç©ºå­—å…¸åˆ›å»ºå®ä¾‹
            try:
                return self.output_schema()
            except:
                # å¦‚æœè¿ç©ºå®ä¾‹éƒ½æ— æ³•åˆ›å»ºï¼Œæ„é€ ä¸€ä¸ªæœ€å°åŒ–çš„å®ä¾‹
                return self.output_schema(**{field_name: "" if field_info.annotation == str else None 
                                            for field_name, field_info in self.output_schema.model_fields.items() 
                                            if field_info.is_required()})
    
    def _extract_json_from_response(self, text: str) -> str:
        """
        ä»å“åº”æ–‡æœ¬ä¸­æå–JSONå­—ç¬¦ä¸²
        """
        import re
        
        # å¦‚æœåŒ…å«markdownä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSON
        if "```json" in text or "```" in text:
            # æŸ¥æ‰¾JSONå¯¹è±¡çš„è¾¹ç•Œ
            start_index = text.find('{')
            end_index = text.rfind('}')
            if start_index != -1 and end_index != -1 and end_index > start_index:
                text = text[start_index:end_index + 1]
            else:
                # ç§»é™¤markdownæ ‡è®°
                text = text.replace('```json', '').replace('```', '').strip()
        else:
            text = text.strip()
        
        # æ¸…ç†å¸¸è§çš„æ ¼å¼é—®é¢˜
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        # ä¿®å¤æ— æ•ˆçš„åæ–œæ è½¬ä¹‰
        # ä¿ç•™åˆæ³•çš„JSONè½¬ä¹‰: \", \\, \/, \b, \f, \n, \r, \t, \uXXXX
        text = re.sub(r'\\(?!["\\/bfnrtu])', '', text)
        
        # ä¿®å¤è¡¨æ ¼æ ¼å¼ä¸­çš„å¸¸è§é—®é¢˜
        text = text.replace('|\\ ', '| ').replace('|\\', '|')
        
        return text
    
    def _aggressive_json_cleanup(self, text: str) -> str:
        """
        æ›´æ¿€è¿›çš„JSONæ¸…ç†ç­–ç•¥
        """
        import re
        
        # å…ˆè¿›è¡ŒåŸºæœ¬æå–
        json_str = self._extract_json_from_response(text)
        
        # å°è¯•ä¿®å¤ç¼ºå¤±çš„é€—å·
        json_str = re.sub(r'(?<=")\s*(?="[a-zA-Z_])', r',', json_str)
        
        # ä¿®å¤è¡¨æ ¼æ ¼å¼é—®é¢˜
        json_str = re.sub(r'\|\s*\\\s*\|', '| |', json_str)
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ç§»é™¤æ‰€æœ‰åæ–œæ 
        if '\\' in json_str:
            json_str = json_str.replace('\\', '')
        
        return json_str